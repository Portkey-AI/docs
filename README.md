# What is Portkey?

Portkey.ai enables you to build and ship generative AI applications, products and features with confidence.

It’s exciting to get a proof-of-concept up and running for an LLM app, but taking it to production and scaling it for diverse sets of users can be painful.

Portkey wants to make it easy to work with Large Models that expose an API by offering a hosted middleware to

* View all your prompts & generations in real-time on a beautiful dashboard for logs and metrics
* Manage prompt templates, parameters and engines (together called models) & access them via your private model APIs
* Version your models and deploy as needed

Imagine portkey as a hop away from the LLMs enabling you with a lot of functionality out of the box.

![Portkey diagram](roh26it/portkey-quick-start/images/portkey-middleware.png)

#PromptOps #LLMOps #LMOps

#Key Features

Portkey does 2 things really really well: Model management and observability.

### 1.Model management

* Manage your models (prompt, parameters, engines, versions)
* View traffic and latency across models & versions.
* Upgrade seamlessly without downtimes.
* Multi-modal (text, speech, image, language)

### 2.Logs & Monitoring

* View & debug requests in real-time.
* Track traffic & usage across users.
* Get status updates when AI providers go down
* Reduce latency through caching & edge compute

We’re adding more features fast, but key areas of focus for Portkey will be high reliability & low latency.

## Get Started

The best part about Portkey is that you can get started with minimal effort. Just replace your OpenAI (or any other provider’s) endpoint to the portkey proxy and we’ll get you setup within minutes!

