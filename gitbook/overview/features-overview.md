# âœ¨ Features Overview

**Portkey** is building a full-stack LLMOps platform with a singular aim to bring your LLMs to production.

We do that with powerful features across 6 categories:

1. Key Management:&#x20;
2. AI Gateway
3. Observability
4. Prompt Management
5. Experimentation & Evals
6. Security & Compliance



\


* [**Request Tracing**](../key-features/request-tracing.md): Monitor the journey of your requests to identify potential issues and bottlenecks.
* [**Caching**](../key-features/request-caching.md): Improve your application's response time and LLM costs with Simple and Semantic Cache.
* [**Custom Metadata**](../key-features/custom-metadata.md): Filter through logs with custom metadata for better insights.
* [**Automatic Retries**](../key-features/automatic-retries.md): In case of an LLM failure, Portkey automatically retries to ensure uninterrupted service.
* [**Fallbacks on LLMs**](../key-features/fallbacks-on-llms.md): In case of failures, Portkey can automatically switch to a fallback LLM.
* [**Load Balancing**](../key-features/load-balancing.md): Distribute network traffic efficiently across multiple LLMs or multiple API Keys to ensure high availability and reliability.
* [**Observability**](../why-portkey/observability.md)**:** Gain deep insights into the performance and health of your AI applications. Monitor tokens, cost, latency for each LLM request.
