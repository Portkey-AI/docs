# âœ¨ Key Features & Overview



* **Request Tracing**: Monitor the journey of your requests to identify potential issues and bottlenecks.
* **Request Caching**: Improve your application's response time and LLM costs with Simple Cache and Semantic Cache features.
* **Automatic Retries**: In case of an LLM failure, Portkey automatically retries to ensure uninterrupted service.
* **Custom Metadata**: Filter each log through custom passed metadata for better insights.
* **Fallbacks on LLMs**: In case of failure, Portkey can automatically switch to a fallback LLM.
* **Load Balancing**: Distribute network traffic efficiently across multiple LLMs or multiple API Keys to ensure high availability and reliability.
* **Observability:** Gain deep insights into the performance and health of your AI applications. Monitor tokens, cost, latency for each LLM request.

To delve deeper into each feature, check out our Key Features section.
